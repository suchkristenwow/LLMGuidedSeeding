1. Activate the robot's RGB cameras and GPS system for accurate navigation and obstacle avoidance.
2. Use the lidar-inertial odometry to measure the current position of the robot relative to the starting point.
3. Direct the robot to scan the area using its RGB cameras to identify the 'concrete' landmarks as per the 'goal_lms' in the constraint dictionary.
4. Once a 'concrete' area is identified, direct the robot to navigate towards it using the GPS and lidar-inertial odometry data.
5. After reaching the 'concrete' area, instruct the robot to overlay a virtual 'grid' pattern on the 'concrete' area with 'pattern_offset' of 1m x 1m as per the 'pattern' and 'pattern_offset' in the constraint dictionary.
6. Direct the robot to start at one corner of the grid and begin the seeding process.
7. If the 'seed' boolean in the constraint dictionary is 'True', instruct the robot to plant a seed at the current grid intersection.
8. Add the current planting location to the system memory for future reference.
9. Move to the next grid intersection point, navigating around any obstacles or 'avoid' objects that are detected.
10. Repeat steps 7 to 9 until all grid intersection points on the 'concrete' area have been covered and the seeds have been planted as per the defined pattern and density.
11. Once all seeds are planted, guide the robot back to its starting point using the GPS and lidar-inertial odometry data.
12. Finally, perform a final scan to ensure that the task has been completed as per the given prompt and constraint dictionary. If any inconsistencies or issues are detected, alert the human operator for further instructions.