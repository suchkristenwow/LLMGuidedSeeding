initting the Policy Generator with these arguments: 
Namespace(config_path='/home/miles/projects/LLMGuidedSeeding/configs/example_config.toml', logging_dir='/home/miles/projects/LLMGuidedSeeding/experiment_data/0b7e725a-14aa-4af5-a283-a8748ca69b87/policy_generation_logs', plot_bounds_path='/home/miles/projects/LLMGuidedSeeding/random_path.csv', prompt_path='/home/miles/projects/LLMGuidedSeeding/prompts/ex_query.txt')

parsing prompt to get constraints ...
Enhanced Query: You are guiding an autonomous robotic agent to execute the prompt given below. The robot is supposed to help ecologists with tasks related to revegetation of degraded 
rangelands.

The robot is able to observe landmarks via RGB cameras and it is able to plant seeds. 

Given the following prompt, identify any relevant constraints or goal landmarks in the form of a dictionary so that I can write a policy to execute the desired task. 

The keys to this dictionary should be the strings "avoid","goal_lms","pattern","landmark_offset","search", "seed", and "pattern_offset". Here, seed is a string of a boolean variable 
which is true if the robot should plant in the specified pattern. 

For example, if the prompt was: "Plant 10 cm away from any shrubs in the plot", return {"goal_lms":"shrub","landmark_offset":0.1,"seed":"True"}. Or, if the prompt was 
"Seed along the fenceline in a straight row, 15 cm apart", return {"goal_lms":"fence","pattern_offset":0.15,"pattern":"line","seed":"True"}. 
Finally, the prompt: "Count how many shrubs are in the plot." would return {"search":"shrubs","seed":"False"}.

If you're not sure how to format the constraints into this dictionary format, return a question in the form of a string that would help allow you to parse the prompt 
into the dictionary format. An example of a follow-up question could be "I'm sorry, I'm not sure how to parse your prompt. Currently, I want to format the constraints of 
your prompt into a dictionary with the keys: "avoid","goal_lms","pattern","landmark_offset","search", and "pattern_offset". Do you think your prompt constraints can be categorized 
like this or should I edit my constraint dictionary format?"

Prompt: Plant in 1mx1m grid in the bounded area. Avoid driving over wherever youâ€™ve planted or any conmods.


llm_result: {"avoid":"planted area, conmods","pattern":"grid","pattern_offset":1,"seed":"True"}

results_str:  {"avoid":"planted area, conmods","pattern":"grid","pattern_offset":1,"seed":"True"}
constraints:  {'avoid': 'planted area, conmods', 'pattern': 'grid', 'pattern_offset': 1, 'seed': True}
building policy...
feedback is none!
Policy:  1. Check if the robot is within the operational bounds specified in the "plot_bounds". If not, calculate the shortest route to enter the plot without violating any "avoid" constraints.

2. Begin to map out a 1m x 1m grid pattern within the plot bounds, taking care to avoid any "planted area" or "conmods".

3. Starting from the nearest grid point, move to that location while avoiding any areas specified in the "avoid" entry.

4. Upon reaching the grid point, if the "seed" boolean is True, plant a seed. Add this location to the list of planted locations in the system memory.

5. Proceed to the next nearest unvisited grid point, again avoiding any areas specified in the "avoid" entry. Repeat steps 4 and 5 until all grid points within the plot bounds have been visited and seeded.

6. If the robot encounters a landmark in the "goal_lms" which it can't locate in the immediate environment, interface with the human user and ask: "I can't find {landmark}, where should I look?"

7. After all grid points have been visited and seeds planted where necessary, return to the starting position while avoiding all areas specified in the "avoid" entry.

8. Notify the user upon successful completion of the task.
Feedback: enhanced verification response:  The following paragraph is feedback from the user on a policy. Based on the following response, does the user approve of this policy? Return "True" if so, and "False" otherwise.
If the user suggests any changes, return False. 
looks good
verifying the policy; this is the llm result: True
Found a valid policy approved by the human!
